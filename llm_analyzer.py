import os
import json
import logging
from openai import OpenAI, NotFoundError
from oid_to_str import oid_to_str

logger = logging.getLogger("llm_analyzer")
logging.basicConfig(level=logging.INFO)

def analyze_match(jobs: list, student_data: list) -> str:
    print("â–¶ï¸ Starting analyze_match")

    # 1. Load API key
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    print(f"ğŸ”‘ OPENROUTER_API_KEY: {openrouter_key!r}")
    if not openrouter_key:
        print("âŒ Missing OPENROUTER_API_KEY")
        return "âŒ OPENROUTER_API_KEY is missing."

    # 2. Sanitize input
    print(f"ğŸ—‚ Received {len(jobs)} job(s) and {len(student_data)} student record(s)")
    if not student_data:
        print("âŒ student_data empty")
        return "âŒ No student data provided."
    jobs_clean = [oid_to_str(job) for job in jobs if isinstance(job, dict)]
    print(f"ğŸ§¹ jobs_clean length: {len(jobs_clean)}")
    if not jobs_clean:
        print("âŒ No valid jobs after sanitize")
        return "âŒ No jobs to analyze."
    student_clean = oid_to_str(student_data[0])
    print(f"ğŸ‘¤ student_clean keys: {list(student_clean.keys())}")

    # 3. Build prompts
    jobs_json    = json.dumps(jobs_clean, indent=2, separators=(",", ":"))
    student_json = json.dumps(student_clean, indent=2, separators=(",", ":"))
    system_prompt = (
        "You are an expert career advisor and the worldâ€™s most accurate job matcher.\n"
        f"Job Postings:\n{jobs_json}"
    )
    user_prompt = f"Student Profile:\n{student_json}"
    print("âœï¸ Prompts built (system & user)")

    # 4. Instantiate client WITH all required headers (including Authorization)
    print("ğŸ”§ Instantiating OpenRouter client with full headers")
    client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=openrouter_key,
    default_headers={
        "Authorization": f"Bearer {openrouter_key}",
        "HTTP-Referer": "https://v0001-google-production.up.railway.app",
        "X-Title": "JobMatch AI"
        }
    )


    primary_model  = "deepseek/deepseek-r1:free"
    fallback_model = "tngtech/deepseek-r1t-chimera:free"

    # 5. Try primary
    try:
        print(f"ğŸ“¡ Sending to OpenRouter (model: {primary_model})")
        resp = client.chat.completions.create(
            model=primary_model,
            messages=[
                {"role":"system", "content":system_prompt},
                {"role":"user",   "content":user_prompt}
            ],
            temperature=0.1
        )
        print("âœ… Primary model call succeeded")
        return resp.choices[0].message.content

    except NotFoundError as e:
        print(f"âš ï¸ Primary model not found: {e}")

    except Exception as e:
        print(f"âŒ Primary model call exception: {type(e).__name__}: {e}")
        return "âŒ Primary model failed."

    # 6. Try fallback
    try:
        print(f"ğŸ” Retrying with fallback model: {fallback_model}")
        resp = client.chat.completions.create(
            model=fallback_model,
            messages=[
                {"role":"system", "content":system_prompt},
                {"role":"user",   "content":user_prompt}
            ],
            temperature=0.1
        )
        print("âœ… Fallback model call succeeded")
        return resp.choices[0].message.content

    except Exception as e:
        print(f"âŒ Fallback model exception: {type(e).__name__}: {e}")
        return "âŒ Both primary and fallback models failed."
