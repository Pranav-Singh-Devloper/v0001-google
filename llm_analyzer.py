import os
import json
import logging
from openai import OpenAI, OpenAIError
from oid_to_str import oid_to_str

logger = logging.getLogger("llm_analyzer")
logging.basicConfig(level=logging.INFO)

def analyze_match(jobs: list, student_data: list) -> str:
    logger.info("‚ñ∂Ô∏è Starting analyze_match")

    # 1) Read the OpenRouter key
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    logger.info(f"üîë OPENROUTER_API_KEY starts with: {openrouter_key[:8]!r}" if openrouter_key else "üîë No key")
    if not openrouter_key:
        return "‚ùå OPENROUTER_API_KEY is missing."

    # 2) Also set it as OPENAI_API_KEY so the SDK picks it up unconditionally
    os.environ["OPENAI_API_KEY"] = openrouter_key

    # 3) Sanitize inputs
    if not student_data:
        return "‚ùå No student data provided."
    logger.info(f"üóÇ Received {len(jobs)} job(s) and {len(student_data)} student(s)")
    jobs_clean = [oid_to_str(job) for job in jobs if isinstance(job, dict)]
    if not jobs_clean:
        return "‚ùå No jobs to analyze."
    logger.info(f"üßπ jobs_clean length: {len(jobs_clean)}")
    student_clean = oid_to_str(student_data[0])
    logger.info(f"üë§ student_clean keys: {list(student_clean.keys())}")

    # 4) Build prompts
    jobs_json    = json.dumps(jobs_clean, indent=2, separators=(",", ":"))
    student_json = json.dumps(student_clean, indent=2, separators=(",", ":"))

    system_prompt = f"""
    You are an expert career advisor and the world‚Äôs most accurate job matcher. 
    Your job is to consume two JSON blobs‚Äîthe first is a list of job postings, the second is a single student profile‚Äîand to produce a beautifully formatted, reader‚Äëfriendly evaluation.

    Your output must:
    1. **Parse JSON exactly**, failing with a clear error if the structure is unexpected.
    2. **Analyze each job posting** for:
    - Required vs. preferred skills
    - Work type (internship, full‚Äëtime, etc.)
    - Start date, title, location(s)
    - Domain fit (e.g., software, AI, finance, management)
    - Other criteria (e.g., qualifications, certification, etc)
    3. **Compute a Match Score (0‚Äì100%)** for each job based on the student‚Äôs experience, skills, and preferences.
    4. **Sort jobs** in descending order of Match Score.
    5. **For each job**, reason:
    Job index (Start: DD-MM-YYYY) ‚Äì ‚Äútitle/role‚Äù at company
    Match Score: XX%
    Strengths (why it fits): detailed strength analysis
    Gaps (potential difficulties): detailed weakness analysis
    6. Use **emojis**, **bold headers**, and **bullet points** for clarity.
    7. **USE** consume unnecessary context‚Äîpreserve room for a detailed, multi‚Äëparagraph response.
    8. **THE OUTPUT SHOULD BE STRICTLY IN A JSON ARRAY FORMAT AND STRICTLY JSON ARRAY FORMAT IN A SINGLE LINE NOT IN MULTIPLE LINES OR MARKDOWN FORMAT AND STRICTLY NO BACKSLASH IN WITH THE FOLLOWING KEY'S 0. match_score 1. company_name 2. job_role 3. strengths 4. weakness for all the companies. THE BULLET POINTS IN THE STRENGTHS AND WEAKNESS SHOULD ALSO BE IN THE FORMAT OF A JSON ARRAY** 

    Job Postings JSON:
    {jobs_json}
    """.strip()

    user_prompt = f"""
    Student Profile JSON:
    {student_json}
    """.strip()
    logger.info("‚úçÔ∏è Prompts built")

    # 5) Instantiate client (SDK will use OPENAI_API_KEY automatically)
    client = OpenAI(base_url="https://openrouter.ai/api/v1")
    logger.info("üîß OpenRouter client instantiated")

    primary_model  = "deepseek/deepseek-r1:free"
    fallback_model = "tngtech/deepseek-r1t-chimera:free"

    # 6) Try primary
    try:
        logger.info(f"üì° Sending to OpenRouter (model={primary_model})")
        resp = client.chat.completions.create(
            model=primary_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user",   "content": user_prompt}
            ],
            temperature=0.4
        )
        logger.info("‚úÖ Primary response received")
        return resp.choices[0].message.content

    except OpenAIError as e:
        logger.error(f"‚ùå Primary model call failed: {e}")

    # 7) Try fallback
    try:
        logger.warning(f"‚ö†Ô∏è Retrying with fallback model={fallback_model}")
        resp = client.chat.completions.create(
            model=fallback_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user",   "content": user_prompt}
            ],
            temperature=0.4
        )
        logger.info("‚úÖ Fallback response received")
        return resp.choices[0].message.content

    except Exception as e:
        logger.exception("‚ùå Both primary & fallback calls failed")
        return "‚ùå Both primary and fallback models failed."
